<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Architect's Decision Guide: Long-Context vs. Multi-Call RAG</title>
    <style>
        :root {
            --bg: #0f1117;
            --surface: #1a1d27;
            --surface2: #232733;
            --border: #2e3345;
            --text: #e1e4ed;
            --text-muted: #8b90a0;
            --accent: #6c8aff;
            --accent2: #4ecdc4;
            --cost: #f5a623;
            --speed: #e74c8b;
            --accuracy: #4ecdc4;
            --citation: #c084fc;
            --green: #4ade80;
            --red: #f87171;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.7;
        }

        .container {
            max-width: 1100px;
            margin: 0 auto;
            padding: 40px 24px;
        }

        /* ===== NAV ===== */
        nav {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 32px;
            padding-bottom: 16px;
            border-bottom: 1px solid var(--border);
        }

        nav a {
            color: var(--accent);
            text-decoration: none;
            font-size: 0.9rem;
        }

        nav a:hover {
            text-decoration: underline;
        }

        nav .current {
            color: var(--text-muted);
            font-size: 0.9rem;
        }

        /* ===== HEADER ===== */
        header {
            text-align: center;
            margin-bottom: 48px;
            padding-bottom: 32px;
            border-bottom: 1px solid var(--border);
        }

        h1 {
            font-size: 2.2rem;
            font-weight: 700;
            margin-bottom: 12px;
            background: linear-gradient(135deg, var(--accent), var(--accent2));
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
        }

        header p {
            color: var(--text-muted);
            font-size: 1.05rem;
            max-width: 750px;
            margin: 0 auto;
        }

        /* ===== SCENARIOS ===== */
        .scenarios {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin-bottom: 48px;
        }

        .scenario {
            background: var(--surface);
            border: 1px solid var(--border);
            border-radius: 12px;
            padding: 24px;
        }

        .scenario h3 {
            font-size: 1rem;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            margin-bottom: 8px;
        }

        .scenario.lc h3 {
            color: var(--accent);
        }

        .scenario.rag h3 {
            color: var(--accent2);
        }

        .scenario p {
            color: var(--text-muted);
            font-size: 0.95rem;
        }

        /* ===== SECTIONS ===== */
        section {
            margin-bottom: 48px;
        }

        h2 {
            font-size: 1.5rem;
            font-weight: 700;
            margin-bottom: 20px;
            padding-bottom: 8px;
            border-bottom: 2px solid var(--border);
        }

        h2.cost {
            border-color: var(--cost);
        }

        h2.speed {
            border-color: var(--speed);
        }

        h2.accuracy {
            border-color: var(--accuracy);
        }

        h2.citation {
            border-color: var(--citation);
        }

        h2.pipeline {
            border-color: #a78bfa;
        }

        h2.hybrid {
            border-color: var(--accent);
        }

        h2.papers {
            border-color: var(--accent2);
        }

        h3 {
            margin: 20px 0 12px;
        }

        /* ===== FINDINGS ===== */
        .finding {
            background: var(--surface);
            border: 1px solid var(--border);
            border-radius: 10px;
            padding: 20px 24px;
            margin-bottom: 16px;
        }

        .finding h4 {
            font-size: 1rem;
            font-weight: 600;
            margin-bottom: 6px;
        }

        .finding p {
            color: var(--text-muted);
            font-size: 0.93rem;
            margin-bottom: 10px;
        }

        .finding .stat {
            display: inline-block;
            background: var(--surface2);
            border: 1px solid var(--border);
            padding: 4px 12px;
            border-radius: 6px;
            font-size: 0.85rem;
            font-weight: 600;
            margin-right: 8px;
            margin-bottom: 6px;
        }

        .finding .stat.green {
            color: var(--green);
        }

        .finding .stat.red {
            color: var(--red);
        }

        .finding .stat.yellow {
            color: var(--cost);
        }

        .finding .stat.purple {
            color: var(--citation);
        }

        .source-link {
            display: inline-block;
            margin-top: 8px;
            color: var(--accent);
            text-decoration: none;
            font-size: 0.85rem;
            word-break: break-all;
        }

        .source-link:hover {
            text-decoration: underline;
        }

        .source-tag {
            display: inline-block;
            background: var(--surface2);
            color: var(--text-muted);
            font-size: 0.75rem;
            padding: 2px 8px;
            border-radius: 4px;
            margin-left: 8px;
            vertical-align: middle;
        }

        /* ===== TABLES ===== */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 16px 0;
            font-size: 0.9rem;
        }

        th,
        td {
            text-align: left;
            padding: 12px 16px;
            border-bottom: 1px solid var(--border);
        }

        th {
            background: var(--surface2);
            font-weight: 600;
            font-size: 0.8rem;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            color: var(--text-muted);
        }

        td {
            background: var(--surface);
        }

        .tag-lc {
            color: var(--accent);
            font-weight: 600;
        }

        .tag-rag {
            color: var(--accent2);
            font-weight: 600;
        }

        .tag-tie {
            color: var(--cost);
            font-weight: 600;
        }

        /* ===== CALLOUTS ===== */
        .callout {
            background: var(--surface2);
            border-left: 4px solid var(--accent);
            border-radius: 0 8px 8px 0;
            padding: 16px 20px;
            margin: 20px 0;
            font-size: 0.93rem;
        }

        .callout.balanced {
            border-color: var(--cost);
        }

        .callout.hybrid {
            border-color: var(--accent2);
        }

        .callout.pipeline {
            border-color: #a78bfa;
        }

        /* ===== VERDICT GRID ===== */
        .verdict-grid {
            display: grid;
            grid-template-columns: repeat(4, 1fr);
            gap: 16px;
            margin-bottom: 48px;
        }

        .verdict-card {
            background: var(--surface);
            border: 1px solid var(--border);
            border-radius: 12px;
            padding: 24px;
            text-align: center;
        }

        .verdict-card .label {
            font-size: 0.8rem;
            text-transform: uppercase;
            letter-spacing: 0.08em;
            margin-bottom: 8px;
        }

        .verdict-card.cost .label {
            color: var(--cost);
        }

        .verdict-card.speed .label {
            color: var(--speed);
        }

        .verdict-card.accuracy .label {
            color: var(--accuracy);
        }

        .verdict-card.citation .label {
            color: var(--citation);
        }

        .verdict-card .winner {
            font-size: 1.2rem;
            font-weight: 700;
            margin-bottom: 4px;
        }

        .verdict-card .detail {
            color: var(--text-muted);
            font-size: 0.83rem;
        }

        /* ===== PIPELINE DIAGRAM ===== */
        .pipeline-compare {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 20px 0;
        }

        .pipeline-box {
            background: var(--surface);
            border: 1px solid var(--border);
            border-radius: 12px;
            padding: 20px 24px;
        }

        .pipeline-box h4 {
            margin-bottom: 12px;
            font-size: 0.95rem;
        }

        .pipeline-box.lc h4 {
            color: var(--accent);
        }

        .pipeline-box.rag h4 {
            color: var(--accent2);
        }

        .step {
            display: flex;
            align-items: flex-start;
            gap: 10px;
            margin-bottom: 10px;
            padding: 8px 12px;
            background: var(--surface2);
            border-radius: 6px;
            font-size: 0.88rem;
        }

        .step-num {
            flex-shrink: 0;
            width: 22px;
            height: 22px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 0.75rem;
            font-weight: 700;
        }

        .lc .step-num {
            background: var(--accent);
            color: var(--bg);
        }

        .rag .step-num {
            background: var(--accent2);
            color: var(--bg);
        }

        .step-text {
            line-height: 1.4;
            color: var(--text-muted);
        }

        .arrow {
            text-align: center;
            color: var(--text-muted);
            font-size: 0.8rem;
            margin: 4px 0;
        }

        /* ===== DECISION CHECKLIST ===== */
        .decision-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 20px 0;
        }

        .decision-box {
            background: var(--surface);
            border: 1px solid var(--border);
            border-radius: 12px;
            padding: 24px;
        }

        .decision-box h4 {
            margin-bottom: 12px;
            font-size: 1rem;
        }

        .decision-box.lc-wins h4 {
            color: var(--accent);
        }

        .decision-box.rag-wins h4 {
            color: var(--accent2);
        }

        .decision-box ul {
            list-style: none;
            padding: 0;
        }

        .decision-box li {
            padding: 6px 0;
            font-size: 0.9rem;
            color: var(--text-muted);
            border-bottom: 1px solid var(--border);
        }

        .decision-box li:last-child {
            border-bottom: none;
        }

        .decision-box li::before {
            margin-right: 8px;
            font-weight: 600;
        }

        .decision-box.lc-wins li::before {
            content: "→";
            color: var(--accent);
        }

        .decision-box.rag-wins li::before {
            content: "→";
            color: var(--accent2);
        }

        /* ===== SCORECARD ===== */
        .scorecard {
            margin: 24px 0;
        }

        .score-row {
            display: grid;
            grid-template-columns: 200px 1fr 80px 1fr 80px;
            gap: 8px;
            align-items: center;
            padding: 12px 0;
            border-bottom: 1px solid var(--border);
            font-size: 0.9rem;
        }

        .score-row.header {
            font-weight: 600;
            font-size: 0.8rem;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            color: var(--text-muted);
            border-bottom: 2px solid var(--border);
        }

        .score-dimension {
            font-weight: 600;
        }

        .score-detail {
            color: var(--text-muted);
            font-size: 0.85rem;
        }

        .score-value {
            text-align: center;
            font-weight: 700;
            font-size: 0.85rem;
        }

        .score-lc {
            color: var(--accent);
        }

        .score-rag {
            color: var(--accent2);
        }

        /* ===== REF LIST ===== */
        .ref-list {
            list-style: none;
            counter-reset: ref;
        }

        .ref-list li {
            counter-increment: ref;
            background: var(--surface);
            border: 1px solid var(--border);
            border-radius: 8px;
            padding: 16px 20px;
            margin-bottom: 12px;
        }

        .ref-list li::before {
            content: counter(ref) ".";
            font-weight: 700;
            color: var(--accent);
            margin-right: 8px;
        }

        .ref-list li strong {
            display: block;
            margin-bottom: 4px;
        }

        .ref-list li a {
            color: var(--accent);
            text-decoration: none;
            font-size: 0.85rem;
            word-break: break-all;
        }

        .ref-list li a:hover {
            text-decoration: underline;
        }

        .ref-list li .desc {
            color: var(--text-muted);
            font-size: 0.88rem;
            margin-top: 4px;
        }

        /* ===== FOOTER ===== */
        footer {
            text-align: center;
            padding-top: 32px;
            border-top: 1px solid var(--border);
            color: var(--text-muted);
            font-size: 0.82rem;
        }

        /* ===== RESPONSIVE ===== */
        @media (max-width: 900px) {
            .verdict-grid {
                grid-template-columns: repeat(2, 1fr);
            }

            .score-row {
                grid-template-columns: 1fr;
                gap: 4px;
                padding: 16px 0;
            }

            .score-row.header {
                display: none;
            }

            .score-value {
                text-align: left;
            }
        }

        @media (max-width: 700px) {

            .scenarios,
            .verdict-grid,
            .pipeline-compare,
            .decision-grid {
                grid-template-columns: 1fr;
            }

            h1 {
                font-size: 1.6rem;
            }
        }
    </style>
</head>

<body>
    <div class="container">

        <nav>
            <a href="index.html">&larr; Research Index</a>
            <span class="current">Decision Guide</span>
        </nav>

        <header>
            <h1>Which Architecture Should You Choose?</h1>
            <p>An unbiased comparison of <strong>long-context LLM calls</strong> versus
                <strong>multi-call RAG pipelines</strong>. Both approaches must perform the same work&mdash;the
                question is how that work is structured. This guide presents the tradeoffs honestly so you
                can choose the right architecture for your use case.
            </p>
        </header>

        <!-- ===== SCENARIO DEFINITIONS ===== -->
        <div class="scenarios">
            <div class="scenario lc">
                <h3>Long-Context &mdash; One Call</h3>
                <p>Pass the full ~100-page document (~50&ndash;130K tokens) to the LLM along with a prompt
                    containing all task instructions. The model processes everything in a single forward pass,
                    seeing every sentence in the document simultaneously.</p>
            </div>
            <div class="scenario rag">
                <h3>Multi-Call RAG &mdash; Pipeline</h3>
                <p>Use embeddings and retrieval to find relevant paragraphs per task, then make separate
                    LLM calls&mdash;each with a single instruction and only the retrieved context
                    (~500&ndash;2K tokens per call). Tasks may run in parallel or sequentially.</p>
            </div>
        </div>

        <!-- ===== QUICK VERDICT ===== -->
        <div class="verdict-grid">
            <div class="verdict-card cost">
                <div class="label">Cost</div>
                <div class="winner" style="color: var(--accent2);">RAG Cheaper</div>
                <div class="detail">5&ndash;10&times; fewer input tokens. But LC narrows the gap with prompt
                    caching, and RAG has infrastructure overhead.</div>
            </div>
            <div class="verdict-card speed">
                <div class="label">Speed</div>
                <div class="winner" style="color: var(--accent2);">RAG Faster</div>
                <div class="detail">~1s per call (parallelizable) vs. 30&ndash;60s for LC. But LC is simpler
                    to orchestrate&mdash;one call, done.</div>
            </div>
            <div class="verdict-card accuracy">
                <div class="label">Retrieval Accuracy</div>
                <div class="winner" style="color: var(--cost);">Depends on Task</div>
                <div class="detail">LC sees everything (+4&ndash;13% on single-question benchmarks). RAG avoids
                    lost-in-the-middle but can miss context entirely.</div>
            </div>
            <div class="verdict-card citation">
                <div class="label">Citation Ability</div>
                <div class="winner" style="color: var(--accent2);">RAG Wins</div>
                <div class="detail">RAG knows exactly which chunks were retrieved. LC must locate sources across
                    100 pages&mdash;often imprecise.</div>
            </div>
        </div>

        <!-- ===== THE 6 FUNCTIONS ===== -->
        <section>
            <h2 class="pipeline">The 6 Required Functions: Same Work, Different Structure</h2>

            <div class="callout pipeline">
                <strong>Key insight:</strong> Both architectures must perform the same 6 functional tasks. The
                long-context approach embeds all 6 as instructions within a single prompt. The multi-call approach
                separates each into its own LLM call with targeted context. Neither approach eliminates any of
                the work&mdash;the difference is how the work is organized.
            </div>

            <table>
                <thead>
                    <tr>
                        <th>#</th>
                        <th>Functional Task</th>
                        <th>What It Requires</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>1</td>
                        <td><strong>Primary analysis</strong> &mdash; compare source document against reference
                            material</td>
                        <td>Source document + relevant reference/guidance docs</td>
                    </tr>
                    <tr>
                        <td>2</td>
                        <td><strong>Section-level review</strong> &mdash; evaluate specific structural elements</td>
                        <td>Targeted sections + rules applicable to each section type</td>
                    </tr>
                    <tr>
                        <td>3</td>
                        <td><strong>Historical comparison</strong> &mdash; check against past decisions or precedent
                        </td>
                        <td>Historical data (often a different corpus entirely)</td>
                    </tr>
                    <tr>
                        <td>4</td>
                        <td><strong>Citation verification</strong> &mdash; validate references from the primary
                            analysis</td>
                        <td>Output from Task 1 + original reference docs</td>
                    </tr>
                    <tr>
                        <td>5</td>
                        <td><strong>Synthesis &amp; ranking</strong> &mdash; prioritize findings, assign severity</td>
                        <td>Outputs from Tasks 1&ndash;4</td>
                    </tr>
                    <tr>
                        <td>6</td>
                        <td><strong>Output validation</strong> &mdash; check format, bias, prohibited content</td>
                        <td>Output from Task 5 + validation rules</td>
                    </tr>
                </tbody>
            </table>

            <div class="pipeline-compare">
                <div class="pipeline-box lc">
                    <h4>Long-Context: All 6 in One Prompt</h4>
                    <div class="step">
                        <div class="step-num">1</div>
                        <div class="step-text"><strong>Single prompt</strong> contains all 6 instructions plus the
                            entire document (~50&ndash;130K tokens)</div>
                    </div>
                    <div class="arrow">&darr;</div>
                    <div class="step">
                        <div class="step-num">2</div>
                        <div class="step-text"><strong>One LLM call</strong> processes everything in sequence,
                            generating all 6 outputs in a single response</div>
                    </div>
                    <div class="arrow">&darr;</div>
                    <div class="step">
                        <div class="step-num">3</div>
                        <div class="step-text"><strong>Parse the output</strong> to extract each task's results from
                            the monolithic response</div>
                    </div>
                    <p style="color: var(--text-muted); font-size: 0.85rem; margin-top: 12px;">
                        <strong style="color: var(--accent);">Advantage:</strong> Simple orchestration. No retrieval
                        step. Model sees full document context for cross-references.
                    </p>
                    <p style="color: var(--text-muted); font-size: 0.85rem; margin-top: 6px;">
                        <strong style="color: var(--red);">Risk:</strong> Tasks 4&ndash;6 depend on earlier output.
                        The model generates linearly and cannot go back to verify or correct itself. All 6
                        instructions compete for attention.
                    </p>
                </div>
                <div class="pipeline-box rag">
                    <h4>Multi-Call RAG: 6 Separate Calls</h4>
                    <div class="step">
                        <div class="step-num">1</div>
                        <div class="step-text"><strong>Retrieve</strong> relevant paragraphs for each task via
                            embeddings + vector search</div>
                    </div>
                    <div class="arrow">&darr;</div>
                    <div class="step">
                        <div class="step-num">2</div>
                        <div class="step-text"><strong>Tasks 1, 2, 3</strong> run as parallel LLM calls, each with
                            focused context (~500&ndash;2K tokens)</div>
                    </div>
                    <div class="arrow">&darr;</div>
                    <div class="step">
                        <div class="step-num">3</div>
                        <div class="step-text"><strong>Tasks 4 &rarr; 5 &rarr; 6</strong> run sequentially, each
                            receiving prior task output as input</div>
                    </div>
                    <p style="color: var(--text-muted); font-size: 0.85rem; margin-top: 12px;">
                        <strong style="color: var(--accent2);">Advantage:</strong> Each task gets full model attention.
                        Dependent tasks can verify prior output. Failures are isolated.
                    </p>
                    <p style="color: var(--text-muted); font-size: 0.85rem; margin-top: 6px;">
                        <strong style="color: var(--red);">Risk:</strong> Retrieval may miss relevant context.
                        More complex orchestration code. Each task only sees its retrieved snippets, not the full
                        document.
                    </p>
                </div>
            </div>

            <div class="callout balanced">
                <strong>The honest tradeoff:</strong> Long-context gives the model global visibility but forces it to
                juggle 6 tasks simultaneously. Multi-call gives each task focused attention but limits each call's
                visibility to retrieved snippets. The question is whether your tasks need <em>breadth of context</em>
                (favors LC) or <em>depth of focus</em> (favors multi-call).
            </div>
        </section>

        <!-- ===== COST ===== -->
        <section>
            <h2 class="cost">Head-to-Head: Cost</h2>

            <div class="finding">
                <h4>Raw token economics favor RAG significantly</h4>
                <p>A 100-page document is approximately 50,000&ndash;130,000 tokens. Sending this in full means paying
                    for every token as input. A multi-call RAG pipeline retrieves only the relevant paragraphs for each
                    task, typically totaling 6 &times; ~2K = ~12K input tokens. At standard API pricing, this is a
                    <strong>5&ndash;10&times; reduction</strong> in input token cost.
                </p>
                <span class="stat yellow">Long-Context: ~50&ndash;130K input tokens</span>
                <span class="stat green">Multi-Call RAG: ~12K total input tokens</span>
                <br>
                <a class="source-link" href="https://www.meilisearch.com/blog/rag-vs-long-context-llms"
                    target="_blank">meilisearch.com &mdash; RAG vs. Long-Context LLMs</a>
                <span class="source-tag">Meilisearch 2025</span>
            </div>

            <div class="finding">
                <h4>But long-context has cost-reduction tools</h4>
                <p>Prompt caching changes the math for repeated queries against the same document. Anthropic's prefix
                    caching offers <strong>90% cost reduction</strong> on cached input tokens; OpenAI offers
                    <strong>50% automatic caching</strong>. If you're querying the same 100-page document multiple
                    times, the effective per-query cost of long-context drops dramatically after the first call.
                </p>
                <span class="stat green">Anthropic cache: 90% input cost reduction</span>
                <span class="stat green">OpenAI cache: 50% input cost reduction</span>
                <br>
                <a class="source-link" href="https://www.anthropic.com/news/prompt-caching"
                    target="_blank">anthropic.com &mdash; Prompt Caching</a>
                <span class="source-tag">Anthropic</span>
                <br>
                <a class="source-link"
                    href="https://mcginniscommawill.com/posts/2025-11-17-llm-prompt-caching-comparison/"
                    target="_blank">mcginniscommawill.com &mdash; LLM Prompt Caching Comparison</a>
                <span class="source-tag">McGinnis 2025</span>
            </div>

            <div class="finding">
                <h4>RAG has hidden infrastructure costs</h4>
                <p>The multi-call approach is not "just API calls." You need: an embedding model (to vectorize
                    documents), a vector database (to store and query embeddings), chunking and indexing logic, and
                    orchestration code to manage the pipeline. For a single 100-page document, this overhead is
                    trivial. At enterprise scale with thousands of documents, it becomes a significant engineering
                    and operational cost.</p>
                <span class="stat yellow">Embedding: ~$0.001 per 100-page doc</span>
                <span class="stat yellow">Vector DB: $0&ndash;$100+/mo depending on scale</span>
                <br>
                <a class="source-link" href="https://www.elastic.co/search-labs/blog/rag-vs-long-context-model-llm"
                    target="_blank">elastic.co &mdash; RAG vs Long Context Model LLM</a>
                <span class="source-tag">Elasticsearch Labs</span>
            </div>

            <div class="finding">
                <h4>Self-Route hybrid reduces cost 39&ndash;65% vs. pure long-context</h4>
                <p>The Self-Route method (EMNLP 2024) dynamically routes easy queries to RAG and hard queries to
                    long-context. This achieved a <strong>65% cost reduction</strong> for Gemini-1.5-Pro and
                    <strong>39% for GPT-4o</strong> with less than 2.2% accuracy loss. This suggests that many
                    queries don't actually need the full context.
                </p>
                <span class="stat green">Gemini: 65% cost savings, &minus;2.2% accuracy</span>
                <span class="stat green">GPT-4o: 39% cost savings, &minus;0.2% accuracy</span>
                <br>
                <a class="source-link" href="https://arxiv.org/abs/2407.16833"
                    target="_blank">arxiv.org/abs/2407.16833</a>
                <span class="source-tag">Li et al., EMNLP 2024</span>
            </div>

            <table>
                <thead>
                    <tr>
                        <th>Cost Factor</th>
                        <th>Long-Context</th>
                        <th>Multi-Call RAG</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Input tokens per run</strong></td>
                        <td class="tag-lc">50&ndash;130K</td>
                        <td class="tag-rag">~12K total</td>
                    </tr>
                    <tr>
                        <td><strong>Approx. cost per run (no cache)</strong></td>
                        <td class="tag-lc">$0.50&ndash;$2.00+</td>
                        <td class="tag-rag">$0.05&ndash;$0.20</td>
                    </tr>
                    <tr>
                        <td><strong>With prompt caching (repeated use)</strong></td>
                        <td class="tag-lc">$0.05&ndash;$0.20 (cached)</td>
                        <td class="tag-rag">$0.05&ndash;$0.20 (same)</td>
                    </tr>
                    <tr>
                        <td><strong>Infrastructure overhead</strong></td>
                        <td class="tag-lc">None&mdash;just an API call</td>
                        <td class="tag-rag">Embeddings + vector DB + orchestration</td>
                    </tr>
                    <tr>
                        <td><strong>Engineering complexity</strong></td>
                        <td class="tag-lc">Low&mdash;single prompt</td>
                        <td class="tag-rag">Medium&mdash;pipeline, retrieval tuning</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <!-- ===== SPEED ===== -->
        <section>
            <h2 class="speed">Head-to-Head: Speed</h2>

            <div class="finding">
                <h4>RAG latency is dramatically lower per query</h4>
                <p>Benchmarks consistently show RAG queries averaging approximately <strong>1 second</strong> per
                    call, while long-context calls with 100K+ tokens average <strong>30&ndash;60 seconds</strong>.
                    The dominant factor is the prefill stage&mdash;processing all input tokens before generating
                    the first output token. More input tokens = longer wait.</p>
                <span class="stat green">RAG: ~1s avg per call</span>
                <span class="stat red">LC: ~30&ndash;60s for 100K+ tokens</span>
                <br>
                <a class="source-link" href="https://byteiota.com/rag-vs-long-context-2026-retrieval-debate/"
                    target="_blank">byteiota.com &mdash; RAG vs Long Context 2026</a>
                <span class="source-tag">ByteIota 2026</span>
                <br>
                <a class="source-link" href="https://www.meilisearch.com/blog/rag-vs-long-context-llms"
                    target="_blank">meilisearch.com &mdash; RAG vs. Long-Context LLMs</a>
                <span class="source-tag">Meilisearch</span>
            </div>

            <div class="finding">
                <h4>But long-context is simpler to orchestrate</h4>
                <p>A single long-context call requires no pipeline management. You send one request and get one
                    response. The multi-call approach requires orchestration: managing parallel calls, handling
                    failures and retries per step, passing outputs between dependent tasks, and aggregating
                    results. For prototyping and simple use cases, the simplicity of one call has real value.</p>
            </div>

            <div class="finding">
                <h4>Multi-call parallelism enables significant speedup</h4>
                <p>If tasks are independent, they run concurrently. In a 6-task pipeline where tasks 1&ndash;3
                    are independent, wall-clock time is ~1&ndash;2 seconds for the parallel batch plus
                    ~3 seconds for the sequential chain (tasks 4 &rarr; 5 &rarr; 6). Total: <strong>~4&ndash;6
                        seconds</strong> vs. <strong>30&ndash;60 seconds</strong> for the single long-context call.
                    Even fully sequential (6 &times; 1s), it's still ~6 seconds.</p>
                <span class="stat green">Parallel pipeline: ~4&ndash;6s total</span>
                <span class="stat yellow">Sequential pipeline: ~6s total</span>
                <span class="stat red">Long-Context: ~30&ndash;60s</span>
                <br>
                <a class="source-link"
                    href="https://www.tribe.ai/applied-ai/reducing-latency-and-cost-at-scale-llm-performance"
                    target="_blank">tribe.ai &mdash; Reducing Latency and Cost at Scale</a>
                <span class="source-tag">Tribe AI</span>
            </div>

            <div class="finding">
                <h4>Prompt caching reduces long-context latency for repeated queries</h4>
                <p>Anthropic's prefix caching delivers an <strong>85% reduction in time-to-first-token</strong>
                    for cached prompts. If you're repeatedly querying the same document, the effective latency
                    for subsequent calls drops to ~5&ndash;10 seconds. This makes long-context competitive with
                    sequential RAG pipelines for repeated-use scenarios.</p>
                <span class="stat green">85% TTFT reduction with cache hit</span>
                <br>
                <a class="source-link"
                    href="https://introl.com/blog/prompt-caching-infrastructure-llm-cost-latency-reduction-guide-2025"
                    target="_blank">introl.com &mdash; Prompt Caching Infrastructure Guide</a>
                <span class="source-tag">Introl 2025</span>
            </div>

            <table>
                <thead>
                    <tr>
                        <th>Speed Factor</th>
                        <th>Long-Context</th>
                        <th>Multi-Call RAG</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>First-run latency</strong></td>
                        <td class="tag-lc">30&ndash;60s</td>
                        <td class="tag-rag">4&ndash;6s (parallel pipeline)</td>
                    </tr>
                    <tr>
                        <td><strong>Repeated-query latency (cached)</strong></td>
                        <td class="tag-lc">5&ndash;10s (with cache)</td>
                        <td class="tag-rag">4&ndash;6s (same)</td>
                    </tr>
                    <tr>
                        <td><strong>Orchestration complexity</strong></td>
                        <td class="tag-lc">None&mdash;1 API call</td>
                        <td class="tag-rag">Pipeline management required</td>
                    </tr>
                    <tr>
                        <td><strong>Time-to-first-token</strong></td>
                        <td class="tag-lc">Slow (processes all input first)</td>
                        <td class="tag-rag">Fast (~200ms per call)</td>
                    </tr>
                    <tr>
                        <td><strong>Output generation speed</strong></td>
                        <td class="tag-lc">Slower (large KV-cache)</td>
                        <td class="tag-rag">Normal (small context)</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <!-- ===== RETRIEVAL ACCURACY ===== -->
        <section>
            <h2 class="accuracy">Head-to-Head: Retrieval Accuracy</h2>

            <div class="callout balanced">
                <strong>Important framing:</strong> Most benchmark comparisons test a <em>single question</em>
                against both approaches. When the task is "answer one question about this document," long-context
                outperforms RAG by 4&ndash;13%. But when the task is "perform 6 different analyses," multi-instruction
                accuracy decay changes the calculus. Both findings are valid&mdash;the right comparison depends
                on your workload.
            </div>

            <h3 style="color: var(--accent);">Where Long-Context Wins</h3>

            <div class="finding">
                <h4>Single-task accuracy: LC outperforms RAG by 4&ndash;13%</h4>
                <p>The definitive EMNLP 2024 study found that long-context surpassed RAG by
                    <strong>7.6%</strong> (Gemini-1.5-Pro), <strong>13.1%</strong> (GPT-4o), and
                    <strong>3.6%</strong> (GPT-3.5-Turbo) when both approaches answer the same single question.
                    The model benefits from seeing the full document because it can resolve ambiguities, follow
                    cross-references, and understand the broader context of any given passage.
                </p>
                <span class="stat green">LC +7.6% (Gemini)</span>
                <span class="stat green">LC +13.1% (GPT-4o)</span>
                <span class="stat green">LC +3.6% (GPT-3.5-Turbo)</span>
                <br>
                <a class="source-link" href="https://arxiv.org/abs/2407.16833"
                    target="_blank">arxiv.org/abs/2407.16833</a>
                <span class="source-tag">Li et al., EMNLP 2024</span>
            </div>

            <div class="finding">
                <h4>Cross-document reasoning requires full context</h4>
                <p>When a task requires synthesizing information scattered across many pages&mdash;e.g., "how does
                    the conclusion in section 12 relate to the methodology described in section 3?"&mdash;long-context
                    has a structural advantage. RAG may retrieve section 12 but miss section 3 if its embedding
                    similarity isn't high enough for the query. Long-context sees both.</p>
            </div>

            <div class="finding">
                <h4>For 60%+ of queries, both approaches give the same answer</h4>
                <p>The EMNLP 2024 study also found that for over 60% of queries, RAG and LC produce identical
                    results. The accuracy gap only manifests on harder queries requiring broader context.
                    This suggests that for many straightforward extraction tasks, RAG is sufficient.</p>
                <span class="stat yellow">60%+ of queries: identical results</span>
                <br>
                <a class="source-link" href="https://arxiv.org/abs/2407.16833"
                    target="_blank">arxiv.org/abs/2407.16833</a>
                <span class="source-tag">Li et al., EMNLP 2024</span>
            </div>

            <h3 style="color: var(--accent2);">Where Multi-Call RAG Wins</h3>

            <div class="finding">
                <h4>"Lost in the Middle" degrades long-context accuracy by 30%+</h4>
                <p>The landmark Stanford study demonstrated that LLMs perform best when relevant information
                    is at the <strong>beginning or end</strong> of the context. Performance drops
                    <strong>30%+</strong> when the answer is buried in the middle. With a 100-page document,
                    there's a lot of "middle." RAG sidesteps this entirely by placing retrieved content at the
                    beginning of a short context.
                </p>
                <span class="stat red">30%+ accuracy drop for mid-document info</span>
                <br>
                <a class="source-link" href="https://arxiv.org/abs/2307.03172"
                    target="_blank">arxiv.org/abs/2307.03172</a>
                <span class="source-tag">Liu et al., Stanford / TACL 2024</span>
            </div>

            <div class="finding">
                <h4>RAG accuracy is consistent regardless of document size</h4>
                <p>RAG performance remains nearly constant as the source document grows from 2K to 2M tokens.
                    Long-context accuracy degrades as documents get longer, especially past model-specific thresholds
                    (32K for Llama-3.1-405b, 64K for GPT-4). RAG achieves an <strong>82.58% win-rate</strong>
                    over raw LLMs when evaluated across varying context sizes.</p>
                <span class="stat green">RAG: consistent 2K&ndash;2M tokens</span>
                <span class="stat red">LC: degrades past model thresholds</span>
                <br>
                <a class="source-link"
                    href="https://www.legionintel.com/blog/rag-systems-vs-lcw-performance-and-cost-trade-offs"
                    target="_blank">legionintel.com &mdash; RAG Systems vs LCW</a>
                <span class="source-tag">Legion Intel</span>
                <br>
                <a class="source-link" href="https://www.databricks.com/blog/long-context-rag-performance-llms"
                    target="_blank">databricks.com &mdash; Long Context RAG Performance</a>
                <span class="source-tag">Databricks 2024</span>
            </div>

            <h3 style="color: var(--cost);">The Multi-Instruction Factor</h3>

            <div class="finding">
                <h4>Multi-instruction success decays exponentially</h4>
                <p>When a single prompt contains multiple distinct instructions, overall success follows:
                    <strong>P(all correct) = P(single)<sup>n</sup></strong>. Even at 90% per-instruction accuracy,
                    6 instructions yield only <strong>53% overall success</strong>. This is the
                    "Curse of Instructions"&mdash;it doesn't matter how good the model is at each task individually
                    if it must do all 6 at once.
                </p>
                <span class="stat red">90% per-task &rarr; 53% all-correct (6 tasks)</span>
                <span class="stat red">85% per-task &rarr; 38% all-correct (6 tasks)</span>
                <br>
                <a class="source-link" href="https://openreview.net/forum?id=R6q67CDBCH"
                    target="_blank">openreview.net &mdash; Curse of Instructions</a>
                <span class="source-tag">OpenReview 2025</span>
                <br>
                <a class="source-link" href="https://arxiv.org/html/2507.11538v1"
                    target="_blank">arxiv.org &mdash; How Many Instructions Can LLMs Follow at Once?</a>
                <span class="source-tag">Jaroslawicz et al., 2025</span>
            </div>

            <div class="finding">
                <h4>This applies to long-context regardless of retrieval quality</h4>
                <p>Multi-instruction degradation is independent of the "lost in the middle" problem&mdash;it's about
                    instruction-following, not context retrieval. Even if the model perfectly attends to every token in
                    the 100-page document, cramming 6 diverse tasks into one prompt still causes exponential accuracy
                    decay. A multi-call pipeline isolates each task, giving each instruction the model's full
                    attention.</p>
                <span class="stat yellow">GPT-4o: 15% success at 10 instructions</span>
                <span class="stat yellow">Claude 3.5: 44% success at 10 instructions</span>
                <br>
                <a class="source-link" href="https://www.mdpi.com/2079-9292/14/21/4349"
                    target="_blank">mdpi.com &mdash; Multi-Task Prompting Degradation</a>
                <span class="source-tag">Electronics, 2025</span>
            </div>

            <div class="finding">
                <h4>However: you can make 6 long-context calls too</h4>
                <p>The multi-instruction problem is not inherent to long-context&mdash;it's inherent to
                    <em>single-prompt, multi-task</em> approaches. You could make 6 separate long-context calls,
                    each with one instruction and the full document. This eliminates the instruction-following
                    problem while retaining full document visibility. The tradeoff: 6&times; the cost and latency
                    of a single long-context call.
                </p>
                <span class="stat yellow">6 LC calls: ~$3&ndash;$12 per run (no cache)</span>
                <span class="stat yellow">6 LC calls: ~3&ndash;6 minutes total latency</span>
            </div>

            <table>
                <thead>
                    <tr>
                        <th>Accuracy Factor</th>
                        <th>Long-Context (1 call, 6 tasks)</th>
                        <th>Long-Context (6 calls, 1 task each)</th>
                        <th>RAG (6 calls, 1 task each)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Per-task accuracy</strong></td>
                        <td class="tag-lc">Highest baseline</td>
                        <td class="tag-lc">Highest baseline</td>
                        <td class="tag-rag">4&ndash;13% lower (single-task)</td>
                    </tr>
                    <tr>
                        <td><strong>All-tasks-correct rate</strong></td>
                        <td style="color: var(--red); font-weight: 600;">38&ndash;53% (decay)</td>
                        <td class="tag-lc">~85&ndash;90%</td>
                        <td class="tag-rag">~80&ndash;87%</td>
                    </tr>
                    <tr>
                        <td><strong>Lost-in-the-middle risk</strong></td>
                        <td class="tag-lc">High (100 pages)</td>
                        <td class="tag-lc">High (100 pages)</td>
                        <td class="tag-rag">None (short context)</td>
                    </tr>
                    <tr>
                        <td><strong>Retrieval miss risk</strong></td>
                        <td class="tag-lc">None</td>
                        <td class="tag-lc">None</td>
                        <td style="color: var(--red); font-weight: 600;">Present&mdash;depends on retrieval quality</td>
                    </tr>
                    <tr>
                        <td><strong>Cross-reference ability</strong></td>
                        <td class="tag-lc">Full document visible</td>
                        <td class="tag-lc">Full document visible</td>
                        <td class="tag-rag">Limited to retrieved chunks</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <!-- ===== CITATION ===== -->
        <section>
            <h2 class="citation">Head-to-Head: Citation &amp; Source Attribution</h2>

            <div class="finding">
                <h4>RAG has a structural advantage for citation</h4>
                <p>In a RAG pipeline, you know exactly which chunks were retrieved for each task. Every claim
                    in the LLM's output can be traced back to specific paragraphs with page numbers, section
                    headers, and character offsets. This provenance is <em>architectural</em>&mdash;it comes from
                    the retrieval step, not from the LLM's generation. The LLM doesn't need to "remember"
                    where it found something; the system already recorded it.</p>
                <span class="stat green">RAG: citation via retrieval metadata</span>
                <span class="stat green">Traceable to specific chunks and page numbers</span>
            </div>

            <div class="finding">
                <h4>Long-context citation is possible but less reliable</h4>
                <p>LLMs can be prompted to cite sources from long contexts (e.g., "include the page number and
                    section for each finding"). In practice, this works reasonably well for prominent, clearly
                    delineated sections but becomes unreliable for specific details buried in long documents.
                    The model may cite approximately correct locations or hallucinate page numbers. There's
                    no external system verifying the citations.</p>
                <span class="stat yellow">LC: citation via generation (model must locate source)</span>
                <span class="stat red">Risk of hallucinated page numbers/sections</span>
            </div>

            <div class="finding">
                <h4>SummHay benchmark: citation remains hard for both</h4>
                <p>The "Summary of a Haystack" benchmark (EMNLP 2024) found that even with oracle retrieval,
                    systems performing summarization-with-citation lag estimated human performance (56%) by 10+
                    points. Long-context LLMs without retrieval scored <strong>below 20%</strong> on citation
                    accuracy. This suggests that while RAG has a structural edge, citation is genuinely
                    difficult for both approaches on complex tasks.</p>
                <span class="stat red">LC without retrieval: &lt;20% citation accuracy</span>
                <span class="stat yellow">RAG with oracle retrieval: ~46% citation accuracy</span>
                <br>
                <a class="source-link" href="https://arxiv.org/abs/2407.01370"
                    target="_blank">arxiv.org/abs/2407.01370</a>
                <span class="source-tag">SummHay, EMNLP 2024</span>
            </div>

            <div class="finding">
                <h4>For auditable, regulated environments: RAG is safer</h4>
                <p>In healthcare, legal, and financial compliance, teams need to trace which rule or regulation
                    flagged which issue. A single long-context output is a black box&mdash;the model says
                    "this violates regulation X" but proving it correctly identified and located that regulation
                    is difficult. A RAG pipeline produces separate, logged outputs with known retrieval provenance
                    for each step&mdash;critical for regulatory review.</p>
                <br>
                <a class="source-link" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12407621/"
                    target="_blank">PMC &mdash; AI Agents in Clinical Medicine</a>
                <span class="source-tag">PMC/NIH 2025</span>
                <br>
                <a class="source-link" href="https://aclanthology.org/2024.bionlp-1.4.pdf"
                    target="_blank">aclanthology.org &mdash; Multi-Agent System for Medical Necessity</a>
                <span class="source-tag">BioNLP 2024</span>
            </div>

            <table>
                <thead>
                    <tr>
                        <th>Citation Factor</th>
                        <th>Long-Context</th>
                        <th>Multi-Call RAG</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Citation mechanism</strong></td>
                        <td class="tag-lc">Model generates citations during output</td>
                        <td class="tag-rag">Retrieval system provides source metadata</td>
                    </tr>
                    <tr>
                        <td><strong>Citation reliability</strong></td>
                        <td class="tag-lc">Approximate&mdash;may hallucinate locations</td>
                        <td class="tag-rag">Exact&mdash;tied to retrieved chunk IDs</td>
                    </tr>
                    <tr>
                        <td><strong>Audit trail</strong></td>
                        <td class="tag-lc">Single output, difficult to decompose</td>
                        <td class="tag-rag">Per-task logs with retrieval provenance</td>
                    </tr>
                    <tr>
                        <td><strong>Cross-reference citations</strong></td>
                        <td class="tag-lc">Can cite relationships across distant sections</td>
                        <td class="tag-rag">Limited to retrieved chunks per task</td>
                    </tr>
                    <tr>
                        <td><strong>Verification feasibility</strong></td>
                        <td class="tag-lc">Must search full document to verify</td>
                        <td class="tag-rag">Check specific retrieved paragraphs</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <!-- ===== DECISION FRAMEWORK ===== -->
        <section>
            <h2 class="hybrid">Decision Framework: When Each Approach Wins</h2>

            <div class="callout hybrid">
                <strong>There is no universal winner.</strong> The LaRA benchmark (ICML 2025) tested 2,326 cases
                across 11 LLMs and concluded that the optimal choice depends on "a complex interplay of model
                capabilities, context length, task type, and retrieval characteristics." Use the checklist below
                to evaluate your specific situation.
            </div>

            <div class="decision-grid">
                <div class="decision-box lc-wins">
                    <h4>Choose Long-Context When&hellip;</h4>
                    <ul>
                        <li>Your task requires synthesizing information across the entire document</li>
                        <li>You need holistic understanding (tone, themes, contradictions)</li>
                        <li>The document is short enough to stay within the model's effective threshold</li>
                        <li>You're asking 1&ndash;2 questions, not 6+ distinct tasks</li>
                        <li>You'll query the same document repeatedly (prompt caching helps)</li>
                        <li>Engineering simplicity matters more than cost optimization</li>
                        <li>Cross-references between distant sections are critical</li>
                        <li>Your retrieval quality is poor or untested</li>
                    </ul>
                </div>
                <div class="decision-box rag-wins">
                    <h4>Choose Multi-Call RAG When&hellip;</h4>
                    <ul>
                        <li>You have multiple distinct tasks to perform (3+)</li>
                        <li>Tasks need different context sources (e.g., legal docs + historical data)</li>
                        <li>Later tasks depend on earlier task outputs</li>
                        <li>You need precise citations and audit trails</li>
                        <li>Cost per query is a primary constraint</li>
                        <li>Latency must be under ~10 seconds</li>
                        <li>Error isolation matters (one failing task shouldn't ruin everything)</li>
                        <li>The document exceeds the model's effective context threshold</li>
                        <li>You're in a regulated industry requiring traceability</li>
                    </ul>
                </div>
            </div>

            <h3 style="color: var(--cost);">The Hybrid Option</h3>

            <div class="finding">
                <h4>You don't have to choose one approach exclusively</h4>
                <p>The Self-Route method and similar hybrids dynamically route between RAG and long-context based
                    on query complexity. For your 6-task pipeline, a practical hybrid might look like:</p>
                <p><strong>Tasks 1 &amp; 2</strong> (primary analysis, section review): Use long-context if cross-references
                    are important, or RAG if the relevant sections are clearly identifiable.</p>
                <p><strong>Task 3</strong> (historical comparison): Almost always RAG&mdash;this queries a different
                    corpus entirely.</p>
                <p><strong>Tasks 4, 5, 6</strong> (verification, synthesis, validation): Always separate calls&mdash;they
                    depend on prior outputs and cannot be part of the original call regardless of approach.</p>
                <br>
                <a class="source-link" href="https://arxiv.org/abs/2407.16833"
                    target="_blank">arxiv.org/abs/2407.16833 &mdash; Self-Route</a>
                <span class="source-tag">Li et al., EMNLP 2024</span>
                <br>
                <a class="source-link" href="https://arxiv.org/abs/2502.09977"
                    target="_blank">arxiv.org/abs/2502.09977 &mdash; LaRA</a>
                <span class="source-tag">ICML 2025</span>
            </div>

            <h3 style="color: var(--text);">Final Scorecard</h3>

            <table>
                <thead>
                    <tr>
                        <th>Dimension</th>
                        <th>Long-Context (1 call)</th>
                        <th>Long-Context (6 calls)</th>
                        <th>Multi-Call RAG</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Cost per run</strong></td>
                        <td class="tag-lc">$0.50&ndash;$2.00</td>
                        <td style="color: var(--red); font-weight: 600;">$3.00&ndash;$12.00</td>
                        <td class="tag-rag">$0.05&ndash;$0.20</td>
                    </tr>
                    <tr>
                        <td><strong>Latency</strong></td>
                        <td class="tag-lc">30&ndash;60s</td>
                        <td style="color: var(--red); font-weight: 600;">3&ndash;6 min</td>
                        <td class="tag-rag">4&ndash;6s</td>
                    </tr>
                    <tr>
                        <td><strong>Per-task accuracy</strong></td>
                        <td class="tag-lc">Highest</td>
                        <td class="tag-lc">Highest</td>
                        <td class="tag-rag">4&ndash;13% lower</td>
                    </tr>
                    <tr>
                        <td><strong>All-tasks-correct</strong></td>
                        <td style="color: var(--red); font-weight: 600;">38&ndash;53%</td>
                        <td class="tag-lc">~85&ndash;90%</td>
                        <td class="tag-rag">~80&ndash;87%</td>
                    </tr>
                    <tr>
                        <td><strong>Citation reliability</strong></td>
                        <td class="tag-lc">Low&ndash;moderate</td>
                        <td class="tag-lc">Low&ndash;moderate</td>
                        <td class="tag-rag">High (retrieval metadata)</td>
                    </tr>
                    <tr>
                        <td><strong>Cross-document reasoning</strong></td>
                        <td class="tag-lc">Full visibility</td>
                        <td class="tag-lc">Full visibility</td>
                        <td class="tag-rag">Limited to chunks</td>
                    </tr>
                    <tr>
                        <td><strong>Error isolation</strong></td>
                        <td style="color: var(--red); font-weight: 600;">None</td>
                        <td class="tag-lc">Per-call</td>
                        <td class="tag-rag">Per-call</td>
                    </tr>
                    <tr>
                        <td><strong>Engineering complexity</strong></td>
                        <td class="tag-lc">Low</td>
                        <td class="tag-tie">Medium</td>
                        <td class="tag-tie">Medium&ndash;High</td>
                    </tr>
                    <tr>
                        <td><strong>Infrastructure needed</strong></td>
                        <td class="tag-lc">API key only</td>
                        <td class="tag-lc">API key only</td>
                        <td class="tag-rag">Embeddings + vector DB</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <!-- ===== SOURCES ===== -->
        <section>
            <h2 class="papers">Sources Cited in This Guide</h2>
            <p style="color: var(--text-muted); margin-bottom: 20px;">All research, benchmarks, and technical
                references used in this comparison.</p>

            <h3 style="color: var(--accent);">Academic Research</h3>
            <ol class="ref-list">
                <li>
                    <strong>Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid
                        Approach</strong>
                    <a href="https://arxiv.org/abs/2407.16833" target="_blank">arxiv.org/abs/2407.16833</a>
                    <div class="desc">Li et al., EMNLP 2024. Head-to-head comparison: LC outperforms RAG by
                        4&ndash;13% on average for single questions. Proposes Self-Route hybrid (39&ndash;65%
                        cost reduction, &lt;2.2% accuracy loss).</div>
                </li>
                <li>
                    <strong>Lost in the Middle: How Language Models Use Long Contexts</strong>
                    <a href="https://arxiv.org/abs/2307.03172" target="_blank">arxiv.org/abs/2307.03172</a>
                    <div class="desc">Liu et al., Stanford / TACL 2024. 30%+ accuracy degradation when relevant
                        information is in the middle of long contexts.</div>
                </li>
                <li>
                    <strong>LaRA: Benchmarking RAG and Long-Context LLMs&mdash;No Silver Bullet</strong>
                    <a href="https://arxiv.org/abs/2502.09977" target="_blank">arxiv.org/abs/2502.09977</a>
                    <div class="desc">ICML 2025. 2,326 cases, 11 LLMs. Neither approach dominates; optimal choice
                        depends on model, task, and retrieval quality.</div>
                </li>
                <li>
                    <strong>Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems</strong>
                    <a href="https://arxiv.org/abs/2407.01370" target="_blank">arxiv.org/abs/2407.01370</a>
                    <div class="desc">EMNLP 2024. LC scores below 20% on citation tasks without retrieval.
                        Even oracle RAG lags human performance by 10+ points.</div>
                </li>
                <li>
                    <strong>Curse of Instructions: LLMs Cannot Follow Multiple Instructions at Once</strong>
                    <a href="https://openreview.net/forum?id=R6q67CDBCH"
                        target="_blank">openreview.net/forum?id=R6q67CDBCH</a>
                    <div class="desc">OpenReview 2025. P(all correct) = P(single)^n. 90% per-task &rarr; 53%
                        all-correct for 6 tasks.</div>
                </li>
                <li>
                    <strong>How Many Instructions Can LLMs Follow at Once?</strong>
                    <a href="https://arxiv.org/html/2507.11538v1"
                        target="_blank">arxiv.org/html/2507.11538v1</a>
                    <div class="desc">Jaroslawicz et al., 2025. IFScale benchmark: three degradation patterns
                        (threshold, linear, exponential) across model families.</div>
                </li>
                <li>
                    <strong>Degradation of Multi-Task Prompting Across Six NLP Tasks</strong>
                    <a href="https://www.mdpi.com/2079-9292/14/21/4349"
                        target="_blank">mdpi.com/2079-9292/14/21/4349</a>
                    <div class="desc">Electronics 2025. GPT-4o: 15% all-instructions success at 10 instructions.
                        Claude 3.5: 44%.</div>
                </li>
                <li>
                    <strong>AI Agents in Clinical Medicine: A Systematic Review</strong>
                    <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12407621/"
                        target="_blank">pmc.ncbi.nlm.nih.gov/articles/PMC12407621</a>
                    <div class="desc">PMC/NIH 2025. Multi-agent systems outperform monolithic approaches for
                        clinical reasoning and compliance.</div>
                </li>
                <li>
                    <strong>Multi-Agent System for Medical Necessity Justification</strong>
                    <a href="https://aclanthology.org/2024.bionlp-1.4.pdf"
                        target="_blank">aclanthology.org/2024.bionlp-1.4.pdf</a>
                    <div class="desc">BioNLP 2024. Pipeline architecture for healthcare regulatory tasks
                        demonstrates auditability benefits.</div>
                </li>
            </ol>

            <h3 style="color: var(--accent2);">Industry Benchmarks &amp; Technical Analysis</h3>
            <ol class="ref-list">
                <li>
                    <strong>Databricks: Long Context RAG Performance of LLMs</strong>
                    <a href="https://www.databricks.com/blog/long-context-rag-performance-llms"
                        target="_blank">databricks.com/blog/long-context-rag-performance-llms</a>
                    <div class="desc">20 LLM benchmark. Model-specific degradation thresholds: Llama-3.1-405b
                        at 32K, GPT-4 at 64K.</div>
                </li>
                <li>
                    <strong>Meilisearch: RAG vs. Long-Context LLMs</strong>
                    <a href="https://www.meilisearch.com/blog/rag-vs-long-context-llms"
                        target="_blank">meilisearch.com/blog/rag-vs-long-context-llms</a>
                    <div class="desc">Practical comparison with latency measurements (1s RAG vs 45s LC) and cost
                        analysis.</div>
                </li>
                <li>
                    <strong>Legion Intel: RAG Systems vs LCW Performance and Cost Trade-offs</strong>
                    <a href="https://www.legionintel.com/blog/rag-systems-vs-lcw-performance-and-cost-trade-offs"
                        target="_blank">legionintel.com/blog/rag-systems-vs-lcw-performance-and-cost-trade-offs</a>
                    <div class="desc">RAG 82.58% win-rate over raw LLMs. Consistent performance across context
                        sizes 2K&ndash;2M tokens.</div>
                </li>
                <li>
                    <strong>Elasticsearch Labs: RAG vs Long Context Model LLM</strong>
                    <a href="https://www.elastic.co/search-labs/blog/rag-vs-long-context-model-llm"
                        target="_blank">elastic.co/search-labs/blog/rag-vs-long-context-model-llm</a>
                    <div class="desc">Infrastructure cost analysis from a search-platform perspective.</div>
                </li>
                <li>
                    <strong>ByteIota: RAG vs Long Context 2026</strong>
                    <a href="https://byteiota.com/rag-vs-long-context-2026-retrieval-debate/"
                        target="_blank">byteiota.com/rag-vs-long-context-2026-retrieval-debate</a>
                    <div class="desc">2026 analysis of speed (1s vs 45s), cost, and accuracy with current models.</div>
                </li>
                <li>
                    <strong>Tribe AI: Reducing Latency and Cost at Scale</strong>
                    <a href="https://www.tribe.ai/applied-ai/reducing-latency-and-cost-at-scale-llm-performance"
                        target="_blank">tribe.ai/applied-ai/reducing-latency-and-cost-at-scale-llm-performance</a>
                    <div class="desc">Enterprise parallelization strategies and context management.</div>
                </li>
            </ol>

            <h3 style="color: var(--cost);">Prompt Caching &amp; Cost Optimization</h3>
            <ol class="ref-list">
                <li>
                    <strong>Anthropic: Prompt Caching</strong>
                    <a href="https://www.anthropic.com/news/prompt-caching"
                        target="_blank">anthropic.com/news/prompt-caching</a>
                    <div class="desc">90% cost reduction and 85% latency reduction for cached prefixes.</div>
                </li>
                <li>
                    <strong>McGinnis: LLM Prompt Caching Comparison</strong>
                    <a href="https://mcginniscommawill.com/posts/2025-11-17-llm-prompt-caching-comparison/"
                        target="_blank">mcginniscommawill.com &mdash; Prompt Caching Comparison</a>
                    <div class="desc">Hands-on testing of caching across Anthropic and OpenAI with real
                        measurements.</div>
                </li>
                <li>
                    <strong>Introl: Prompt Caching Infrastructure Guide 2025</strong>
                    <a href="https://introl.com/blog/prompt-caching-infrastructure-llm-cost-latency-reduction-guide-2025"
                        target="_blank">introl.com &mdash; Prompt Caching Guide</a>
                    <div class="desc">85% TTFT reduction for repeated long-context calls.</div>
                </li>
            </ol>
        </section>

        <footer>
            <p>Research comparison compiled February 2026. This guide presents both approaches objectively&mdash;the
                right choice depends on your specific requirements. Sources should be independently verified.</p>
            <p style="margin-top: 8px;"><a href="index.html"
                    style="color: var(--accent); text-decoration: none;">&larr; Back to Research Index</a></p>
        </footer>

    </div>
</body>

</html>